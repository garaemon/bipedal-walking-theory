import { MathBlock } from "@/components/math/MathBlock";
import { CodeEditor } from "@/components/code/CodeEditor";
import { InteractiveDemo } from "@/components/visualization/InteractiveDemo";
import { RLLoopDiagram } from "@/components/diagrams/RLDiagram";
import { PPODiagram } from "@/components/diagrams/PPODiagram";

# Reinforcement Learning for Walking

Reinforcement Learning (RL) has revolutionized bipedal locomotion by
enabling robots to discover walking behaviors through trial and error
in simulation, often surpassing hand-designed controllers.

## Why Reinforcement Learning for Walking?

Walking involves complex contact dynamics: feet collide with the ground,
friction forces change discontinuously, and the robot must coordinate
many joints simultaneously. These challenges make walking hard to solve
with traditional approaches alone.

**Model-based methods** (ZMP in Ch 3, MPC in Ch 10) require accurate
dynamic models. In practice, modeling ground contact, joint friction,
and actuator dynamics precisely is difficult. Small model errors
accumulate and cause the robot to fall.

**RL learns directly from interaction** with a physics simulator,
without needing an explicit model. The policy discovers what works
through trial and error. This allows RL to:

- Handle complex contact dynamics that are hard to model analytically
- Discover creative locomotion strategies that engineers would not design by hand
- Generalize across terrains and perturbations when trained with domain randomization

**However, RL has important limitations:**

- **Sample inefficiency**: training may require billions of simulation steps
- **Reward engineering**: poorly designed rewards lead to unnatural or exploitative behaviors
- **Sim-to-real gap**: policies trained in simulation may fail on real hardware (see Ch 12)

The practical trade-off is: RL is general but data-hungry, while model-based
methods are data-efficient but limited by model accuracy. Modern approaches
increasingly combine both (see Connections at the end).

## MDP Formulation for Locomotion

Walking is formulated as a **Markov Decision Process** (MDP):

- **State** $s$: joint angles, velocities, body orientation, contact states
- **Action** $a$: target joint positions or torques
- **Reward** $r(s, a)$: shaped to encourage desired walking behavior
- **Transition** $P(s'|s, a)$: governed by physics simulation

The goal is to find a policy $\pi_\theta(a|s)$ that maximizes expected cumulative reward:

$$
J(\theta) = \mathbb{E}_{\pi_\theta}\left[\sum_{t=0}^{T} \gamma^t r(s_t, a_t)\right]
$$

where $\gamma \in [0, 1)$ is the discount factor that controls how much the agent
cares about future rewards versus immediate ones.

<RLLoopDiagram />

## Reward Function Design

The reward function is critical and typically includes:

$$
r = w_v r_{velocity} + w_e r_{energy} + w_a r_{alive} + w_s r_{style}
$$

| Component | Formula | Purpose |
|-----------|---------|---------|
| Velocity | $r_v = \exp(-\|v - v_{target}\|^2)$ | Track desired speed |
| Energy | $r_e = -\|\boldsymbol{\tau}\|^2$ | Minimize energy use |
| Alive | $r_a = +1$ per step | Encourage survival |
| Style | $r_s = -\|q - q_{ref}\|^2$ | Match reference motion |

### The Art of Reward Engineering

Reward design profoundly affects the learned behavior. Small changes in
reward weights can produce completely different gaits.

**Common failure modes:**

- **Too much velocity reward**: the robot leans forward aggressively and falls after a few steps
- **No energy penalty**: the robot uses excessively large torques, leading to unrealistic and hardware-damaging motions
- **No smoothness penalty**: the resulting gait is jittery with rapid torque oscillations
- **Reward hacking**: the robot finds unexpected ways to maximize reward (e.g., sliding instead of walking, or vibrating to accumulate alive bonuses)
- **Sparse rewards**: "reach the goal" alone gives insufficient learning signal for the agent to discover walking

**Practical guidelines:**

1. Start with a simple reward (alive bonus + forward velocity) and verify basic behavior
2. Add penalty terms incrementally (energy, smoothness, joint limits)
3. Use curriculum learning to increase task difficulty gradually
4. Consider motion imitation rewards (see DeepMimic section below) to avoid most design issues entirely

## Policy Gradient Methods

### PPO (Proximal Policy Optimization)

The most widely used algorithm for locomotion:

$$
L^{CLIP}(\theta) = \mathbb{E}\left[\min\left(r_t(\theta)A_t, \text{clip}(r_t(\theta), 1-\epsilon, 1+\epsilon)A_t\right)\right]
$$

where $r_t(\theta) = \frac{\pi_\theta(a_t|s_t)}{\pi_{\theta_{old}}(a_t|s_t)}$ is the
probability ratio and $A_t$ is the advantage estimate.

**Why clipping?** Without clipping, a large policy update could drastically change behavior,
causing catastrophic performance collapse. The clip range (typically $\epsilon = 0.2$) defines
a "trust region" around the old policy. If the new policy tries to move too far from the old
one, the clipped objective stops the gradient from pushing further. This ensures stable,
incremental policy improvements.

### Generalized Advantage Estimation (GAE)

The advantage $A_t$ estimates "how much better was this action compared to what we expected."
GAE computes it using a weighted combination of multi-step temporal difference errors:

$$
A_t^{GAE} = \sum_{l=0}^{T-t} (\gamma \lambda)^l \delta_{t+l}
$$

where the TD error at each step is:

$$
\delta_t = r_t + \gamma V(s_{t+1}) - V(s_t)
$$

The parameter $\lambda \in [0, 1]$ controls the bias-variance trade-off:

- $\lambda = 0$: uses only the one-step TD error $\delta_t$. Low variance but high bias (relies heavily on the value function accuracy).
- $\lambda = 1$: uses the full Monte Carlo return. Low bias but high variance (sensitive to individual trajectory noise).
- In practice, $\lambda = 0.95$ is a common choice that balances both.

<PPODiagram />

### On-Policy vs Off-Policy Methods

Two fundamental approaches exist for collecting and using training data:

**PPO (on-policy):** collects a batch of trajectories, updates the policy, then
discards the data. Simple and stable, but requires fresh data each iteration.

**SAC (Soft Actor-Critic, off-policy):** stores all experience in a replay buffer
and reuses old data across many updates. More sample-efficient but harder to tune,
as old data may not reflect the current policy.

**For locomotion, PPO is more popular because:**

1. It is easy to parallelize across thousands of simulation environments
2. Training is more stable and requires less hyperparameter tuning
3. Modern GPU-accelerated simulators (e.g., IsaacGym) can run 4096+ parallel environments, making PPO's sample inefficiency less of a practical issue

<CodeEditor
  initialCode={`import math
import random

# Simplified RL for a 1D walking task
# (CartPole-like balance + forward velocity)
#
# Note: This example uses a simple Evolution Strategy (ES) for
# demonstration. ES is not a gradient-based RL algorithm, but it
# shares the same optimization goal: find policy parameters that
# maximize cumulative reward. True PPO uses policy gradient
# computation with neural network backpropagation, which requires
# deep learning libraries (PyTorch, JAX) not available in this
# browser environment.

random.seed(42)

class SimpleWalker:
    """1D walker: state = [position, velocity, angle, angular_vel]"""
    def __init__(self):
        self.reset()

    def reset(self):
        self.x = 0.0
        self.v = 0.0
        self.theta = random.uniform(-0.1, 0.1)
        self.omega = 0.0
        return self.get_state()

    def get_state(self):
        return [self.x, self.v, self.theta, self.omega]

    def step(self, action):
        """Simulate one time step and return (state, reward, done)."""
        force = action * 10.0
        dt = 0.02
        g = 9.81
        l = 1.0
        m = 1.0
        # Update angle (inverted pendulum dynamics)
        alpha = (g * math.sin(self.theta) + force * math.cos(self.theta)) / l
        self.omega += alpha * dt
        self.theta += self.omega * dt
        # Update position
        self.v += force * dt / m
        self.x += self.v * dt
        # Compute reward
        alive = abs(self.theta) < 0.5
        reward = 0.0
        if alive:
            reward += 1.0
            reward += 0.5 * min(self.v, 1.0)
            reward -= 0.1 * abs(self.theta)
            reward -= 0.01 * action**2
        done = not alive
        return self.get_state(), reward, done

def compute_policy(state, w):
    """Linear policy: action = w . state"""
    action = sum(w[i] * state[i] for i in range(4))
    return max(-1, min(1, action))

def evaluate_policy(env, w, max_steps=200):
    """Run one episode and return total reward."""
    state = env.reset()
    total_reward = 0
    for _ in range(max_steps):
        action = compute_policy(state, w)
        state, reward, done = env.step(action)
        total_reward += reward
        if done:
            break
    return total_reward

# --- Evolution strategy training ---
env = SimpleWalker()
weights = [0.0, 0.0, 0.0, 0.0]
best_weights = weights[:]
best_reward = evaluate_policy(env, best_weights)

print("=== Simple RL Training (Evolution Strategy) ===")
print("Gen  Best_Reward  Avg_Reward  Weights")
print("-" * 60)

n_generations = 30
n_population = 10
noise_std = 0.5

best_reward_history = []
avg_reward_history = []
weight_history = []

for gen in range(n_generations):
    rewards = []
    population = []
    for _ in range(n_population):
        w = [best_weights[i] + random.gauss(0, noise_std) for i in range(4)]
        r = evaluate_policy(env, w)
        rewards.append(r)
        population.append(w)

    avg_r = sum(rewards) / len(rewards)
    best_idx = rewards.index(max(rewards))
    if rewards[best_idx] > best_reward:
        best_reward = rewards[best_idx]
        best_weights = population[best_idx][:]

    best_reward_history.append(best_reward)
    avg_reward_history.append(avg_r)
    weight_history.append(best_weights[:])

    if gen % 5 == 0:
        w_str = [f"{w:.2f}" for w in best_weights]
        print(f" {gen:2d}      {best_reward:7.2f}     {avg_r:7.2f}    {w_str}")

    noise_std *= 0.95

print(f"\\nFinal best reward: {best_reward:.2f}")
print(f"Learned weights: {[f'{w:.3f}' for w in best_weights]}")

# --- Plot learning curves and weight evolution ---
import matplotlib
matplotlib.use("agg")
import matplotlib.pyplot as plt
import numpy as np
import io, base64

generations = list(range(n_generations))

fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(8, 7))

ax1.plot(generations, best_reward_history, "b-o", markersize=3,
         label="Best Reward")
ax1.plot(generations, avg_reward_history, "r--s", markersize=3,
         label="Avg Reward")
ax1.set_xlabel("Generation")
ax1.set_ylabel("Reward")
ax1.set_title("Learning Curve: Evolution Strategy")
ax1.legend()
ax1.grid(True, alpha=0.3)

weight_arr = np.array(weight_history)
labels = ["w_pos", "w_vel", "w_angle", "w_ang_vel"]
for i in range(4):
    ax2.plot(generations, weight_arr[:, i], "-o", markersize=3,
             label=labels[i])
ax2.set_xlabel("Generation")
ax2.set_ylabel("Weight Value")
ax2.set_title("Weight Evolution Over Training")
ax2.legend()
ax2.grid(True, alpha=0.3)

plt.tight_layout()
buf = io.BytesIO()
plt.savefig(buf, format="png", dpi=100, bbox_inches="tight")
buf.seek(0)
img = base64.b64encode(buf.read()).decode("utf-8")
print(f"data:image/png;base64,{img}")
plt.close()
`}
/>

## Simulation: Reward Component Analysis

Different reward components compete during training. By decomposing the
total reward into its constituents (alive bonus, velocity reward, and
energy penalty), we can observe how the agent trades off locomotion
speed against energy efficiency as learning progresses.

<CodeEditor
  initialCode={`import math
import random

# Evolution strategy training with per-component reward tracking.
# We separate each reward term to visualize how the agent balances
# competing objectives over the course of training.

random.seed(42)

class WalkerWithRewardBreakdown:
    """Walker that returns decomposed reward components."""
    def __init__(self):
        self.reset()

    def reset(self):
        self.x = 0.0
        self.v = 0.0
        self.theta = random.uniform(-0.1, 0.1)
        self.omega = 0.0
        return self.get_state()

    def get_state(self):
        return [self.x, self.v, self.theta, self.omega]

    def step(self, action):
        """Return state, total reward, done, and component dict."""
        force = action * 10.0
        dt = 0.02
        g, l, m = 9.81, 1.0, 1.0
        alpha = (g * math.sin(self.theta) + force * math.cos(self.theta)) / l
        self.omega += alpha * dt
        self.theta += self.omega * dt
        self.v += force * dt / m
        self.x += self.v * dt

        alive = abs(self.theta) < 0.5
        if alive:
            r_alive = 1.0
            r_velocity = 0.5 * min(self.v, 1.0)
            r_energy = -0.01 * action**2
        else:
            r_alive = 0.0
            r_velocity = 0.0
            r_energy = 0.0

        total = r_alive + r_velocity + r_energy
        components = {"alive": r_alive, "velocity": r_velocity,
                      "energy": r_energy}
        return self.get_state(), total, not alive, components

def compute_policy(state, w):
    """Linear policy: action = w . state"""
    action = sum(w[i] * state[i] for i in range(4))
    return max(-1, min(1, action))

def evaluate_with_components(env, w, max_steps=200):
    """Run one episode and return total reward + component sums."""
    state = env.reset()
    total_reward = 0.0
    comp_sums = {"alive": 0.0, "velocity": 0.0, "energy": 0.0}
    for _ in range(max_steps):
        action = compute_policy(state, w)
        state, reward, done, comps = env.step(action)
        total_reward += reward
        for k in comp_sums:
            comp_sums[k] += comps[k]
        if done:
            break
    return total_reward, comp_sums

# --- Training loop ---
env = WalkerWithRewardBreakdown()
best_weights = [0.0, 0.0, 0.0, 0.0]
best_reward, _ = evaluate_with_components(env, best_weights)

n_generations = 40
n_population = 10
noise_std = 0.5

alive_history = []
velocity_history = []
energy_history = []

print("=== Reward Component Analysis ===")
print("Gen   Total   Alive  Velocity  Energy")
print("-" * 50)

for gen in range(n_generations):
    rewards = []
    population = []
    comp_list = []
    for _ in range(n_population):
        w = [best_weights[i] + random.gauss(0, noise_std) for i in range(4)]
        r, comps = evaluate_with_components(env, w)
        rewards.append(r)
        population.append(w)
        comp_list.append(comps)

    best_idx = rewards.index(max(rewards))
    if rewards[best_idx] > best_reward:
        best_reward = rewards[best_idx]
        best_weights = population[best_idx][:]

    # Record best individual component breakdown
    _, best_comps = evaluate_with_components(env, best_weights)
    alive_history.append(best_comps["alive"])
    velocity_history.append(best_comps["velocity"])
    energy_history.append(abs(best_comps["energy"]))

    if gen % 5 == 0:
        print(f" {gen:2d}   {best_reward:6.1f}  {best_comps['alive']:5.1f}"
              f"    {best_comps['velocity']:5.1f}"
              f"    {best_comps['energy']:6.2f}")

    noise_std *= 0.95

print(f"\\nFinal best reward: {best_reward:.2f}")

# --- Stacked area chart of reward components ---
import matplotlib
matplotlib.use("agg")
import matplotlib.pyplot as plt
import numpy as np
import io, base64

gens = np.arange(n_generations)
alive_arr = np.array(alive_history)
vel_arr = np.array(velocity_history)
energy_arr = np.array(energy_history)

fig, ax = plt.subplots(figsize=(8, 5))
ax.stackplot(gens, alive_arr, vel_arr, energy_arr,
             labels=["Alive Bonus", "Velocity Reward",
                     "Energy Penalty (abs)"],
             colors=["#4CAF50", "#2196F3", "#FF5722"],
             alpha=0.8)
ax.set_xlabel("Generation")
ax.set_ylabel("Cumulative Reward Component")
ax.set_title("Reward Component Breakdown Over Training")
ax.legend(loc="upper left")
ax.grid(True, alpha=0.3)
plt.tight_layout()

buf = io.BytesIO()
plt.savefig(buf, format="png", dpi=100, bbox_inches="tight")
buf.seek(0)
img = base64.b64encode(buf.read()).decode("utf-8")
print(f"data:image/png;base64,{img}")
plt.close()
`}
/>

## Motion Imitation (DeepMimic)

Instead of learning from scratch, **motion imitation** provides reference
motions from motion capture or animation:

$$
r_{imitation} = w_p \exp(-\|q - q^{ref}\|^2) + w_v \exp(-\|\dot{q} - \dot{q}^{ref}\|^2)
$$

This dramatically improves learning speed and motion quality by guiding
the policy toward natural-looking gaits. Motion imitation also sidesteps
most reward engineering difficulties: the reference motion implicitly
defines what "good walking" looks like, eliminating the need to manually
balance velocity, energy, and style reward weights.

## Curriculum Learning

Complex walking is often learned progressively:

1. **Standing balance**: learn to stay upright
2. **Walking on flat ground**: basic forward locomotion
3. **Varied terrain**: hills, stairs, obstacles
4. **External perturbations**: push recovery
5. **Agile behaviors**: running, jumping, turning

Each stage builds on the policy learned in the previous stage. The key
insight is that a policy trained only on easy tasks generalizes poorly,
while training directly on hard tasks fails because the reward signal
is too sparse for the agent to discover useful behaviors.

## Connections to Other Chapters

- **Sim-to-Real Transfer (Ch 12)**: RL policies are trained in simulation and transferred to real robots. Domain randomization and system identification are essential for bridging the sim-to-real gap.
- **CPG (Ch 7)**: Central Pattern Generators can provide structured action spaces for RL. Instead of learning raw joint torques, the policy can output CPG parameters (frequency, amplitude), reducing the search space.
- **MPC (Ch 10)**: Model-based methods like MPC can be combined with RL. For example, a learned dynamics model can replace the analytical model in MPC, or RL can learn a residual correction on top of an MPC baseline.
- **The trend**: modern locomotion systems increasingly use hybrid approaches that combine model-based structure with learned components. Pure RL and pure model-based methods each have complementary strengths.

## References

- X. Peng et al., "[DeepMimic: Example-Guided Deep Reinforcement Learning of Physics-Based Character Skills](https://arxiv.org/abs/1804.02717)," *ACM TOG*, 2018.
- Z. Xie et al., "[Feedback Control For Cassie With Deep Reinforcement Learning](https://arxiv.org/abs/1803.05580)," *Proc. IROS*, 2018.
- J. Schulman et al., "[Proximal Policy Optimization Algorithms](https://arxiv.org/abs/1707.06347)," *arXiv*, 2017.
- J. Schulman et al., "[High-Dimensional Continuous Control Using Generalized Advantage Estimation](https://arxiv.org/abs/1506.02438)," *ICLR*, 2016.
- T. Haarnoja et al., "[Soft Actor-Critic: Off-Policy Maximum Entropy Deep Reinforcement Learning](https://arxiv.org/abs/1801.01290)," *ICML*, 2018.

<InteractiveDemo title="RL Training Visualization">
  <p className="text-sm text-gray-500">
    Interactive RL training visualization with reward shaping demo coming soon.
  </p>
</InteractiveDemo>
