import { MathBlock } from "@/components/math/MathBlock";
import { CodeEditor } from "@/components/code/CodeEditor";
import { InteractiveDemo } from "@/components/visualization/InteractiveDemo";
import { MPCHorizonDiagram } from "@/components/diagrams/MPCDiagram";
import { MPCConstraintDiagram } from "@/components/diagrams/MPCConstraintDiagram";

# Model Predictive Control (MPC)

Model Predictive Control optimizes control inputs over a future horizon at
each time step, providing a powerful framework for walking that handles
constraints explicitly.

## MPC Formulation

At each time step $k$, solve the optimization:

<MathBlock tex="\min_{u_0, \ldots, u_{N-1}} \sum_{i=0}^{N-1} \left[ \|\mathbf{x}_i - \mathbf{x}_i^{ref}\|^2_Q + \|u_i\|^2_R \right] + \|\mathbf{x}_N - \mathbf{x}_N^{ref}\|^2_{Q_f}" />

subject to:
- Dynamics: $\mathbf{x}_{i+1} = A\mathbf{x}_i + Bu_i$
- State constraints: $\mathbf{x}_i \in \mathcal{X}$
- Input constraints: $u_i \in \mathcal{U}$
- ZMP constraint: $p_i \in \text{support polygon}$

Apply only the first input $u_0^*$, then re-solve at the next step.

<MPCHorizonDiagram />

### Why Receding Horizon?

Why not solve one long-horizon problem at the start and execute the entire plan open-loop?
There are three key reasons:

1. **Model mismatch**: The LIPM is a simplification. The real robot has flexible joints,
   unmodeled friction, and other dynamics the model ignores. Small errors accumulate
   over time.
2. **Disturbances**: External pushes, uneven terrain, or slippery surfaces cannot be
   predicted in advance. Re-solving incorporates the measured state as feedback.
3. **Changing objectives**: The desired footstep plan may change mid-walk (e.g., an
   obstacle appears, or a human gives a new goal). MPC naturally adapts because it
   replans from the current state.

By re-solving at each time step with the latest measured state, MPC continuously
corrects for all of these issues. Think of it like a GPS that recalculates your
route every few seconds, rather than computing it once at the start of your trip.

## Linear MPC with LIPM

Using the LIPM state-space model from Chapter 5, the MPC becomes a
**Quadratic Program** (QP) that can be solved efficiently.

### State-Space Model

$$
\mathbf{x}_{k+1} = A\mathbf{x}_k + Bu_k, \quad p_k = C\mathbf{x}_k
$$

with $\mathbf{x} = [x, \dot{x}, \ddot{x}]^T$, $u = \dddot{x}$ (jerk),
and $p$ the ZMP position.

### QP Formulation

The MPC problem can be written as:

$$
\min_{\mathbf{U}} \frac{1}{2}\mathbf{U}^T H \mathbf{U} + \mathbf{f}^T \mathbf{U}
$$

subject to $A_{ineq}\mathbf{U} \leq \mathbf{b}_{ineq}$

where $\mathbf{U} = [u_0, u_1, \ldots, u_{N-1}]^T$ is the stacked control input.

### Constructing the QP from State-Space Model

To obtain the matrices $H$ and $\mathbf{f}$ concretely, we "unroll" the dynamics
over the prediction horizon of $N$ steps.

**Step 1: Stack the dynamics.**
Starting from $\mathbf{x}_0$, we can write all future states as a function of
the initial state and the control sequence:

$$
\mathbf{X} = \Phi \mathbf{x}_0 + \Gamma \mathbf{U}
$$

where $\mathbf{X} = [\mathbf{x}_1, \mathbf{x}_2, \ldots, \mathbf{x}_N]^T$ is the
stacked state vector, and

$$
\Phi = \begin{bmatrix} A \\ A^2 \\ \vdots \\ A^N \end{bmatrix}, \quad
\Gamma = \begin{bmatrix}
B & 0 & \cdots & 0 \\
AB & B & \cdots & 0 \\
\vdots & \vdots & \ddots & \vdots \\
A^{N-1}B & A^{N-2}B & \cdots & B
\end{bmatrix}
$$

$\Phi$ propagates the initial state forward, and $\Gamma$ is a lower-triangular
matrix that maps control inputs to future states.

**Step 2: Predict ZMP trajectory.**
The ZMP at each step is $p_i = C \mathbf{x}_i$. Stacking all ZMP predictions:

$$
\mathbf{P} = \bar{C} \mathbf{X} = \bar{C} \Phi \mathbf{x}_0 + \bar{C} \Gamma \mathbf{U}
$$

where $\bar{C} = \text{diag}(C, C, \ldots, C)$ is a block-diagonal matrix.

**Step 3: Form the cost function.**
The ZMP tracking cost is:

$$
J = (\mathbf{P} - \mathbf{P}_{ref})^T Q (\mathbf{P} - \mathbf{P}_{ref}) + \mathbf{U}^T R \mathbf{U}
$$

Expanding this and collecting terms in $\mathbf{U}$:

$$
H = \Gamma^T \bar{C}^T Q \bar{C} \Gamma + R
$$

$$
\mathbf{f} = \Gamma^T \bar{C}^T Q (\bar{C} \Phi \mathbf{x}_0 - \mathbf{P}_{ref})
$$

**Small example (N=3):** With a sampling period $T = 0.1$ s, each column
of $\Gamma$ contains the influence of one control input on all future states.
For instance, $u_0$ affects $\mathbf{x}_1$ through $B$, $\mathbf{x}_2$ through
$AB$, and $\mathbf{x}_3$ through $A^2B$. Meanwhile $u_2$ only affects
$\mathbf{x}_3$ through $B$. This lower-triangular structure is key:
earlier control inputs have more influence on the predicted trajectory than
later ones.

## MPC vs. LQR vs. Preview Control

Understanding how MPC relates to other controllers from earlier chapters
clarifies when to use each method.

- **LQR** (Chapter 4) is a special case: it is equivalent to infinite-horizon MPC
  with no constraints. LQR produces a fixed gain matrix $K$ computed offline,
  so there is no online optimization.
- **Preview control** (Chapter 5) is another special case: infinite-horizon with a
  preview window of future references, but still no inequality constraints.
- **MPC's key advantage is constraints.** It can explicitly enforce that the ZMP stays
  inside the support polygon, that step lengths are bounded, etc.
- When no constraints are active, MPC and LQR produce nearly identical control
  inputs. The constraint handling is what makes MPC worth the extra computation.

| Feature | Preview Control | MPC |
|---|---|---|
| Constraints | No | Yes |
| Computation | Offline gain matrices | Online QP at each step |
| Optimality | Infinite horizon | Finite horizon (N steps) |
| Reactivity | Limited (fixed gains) | High (replanning each step) |
| Footstep modification | Cannot change online | Can adapt footsteps online |

<CodeEditor
  initialCode={`import math

# Simple Linear MPC for LIPM walking
# Using unconstrained gradient descent (for demonstration only).
#
# Note: This simplified example uses unconstrained gradient descent
# for illustration. In practice, walking MPC uses dedicated QP solvers
# (qpOASES, OSQP, Clarabel) that handle inequality constraints
# efficiently. Typical computation times are 0.1-5 ms per solve,
# allowing control at 100-1000 Hz.

g = 9.81
z_c = 0.8
T = 0.02  # sampling period
N = 40    # prediction horizon

# LIPM matrices
A = [[1, T, T**2/2], [0, 1, T], [0, 0, 1]]
B = [T**3/6, T**2/2, T]
C = [1, 0, -z_c/g]

def mat_vec_mul(M, v):
    return [sum(M[i][j]*v[j] for j in range(len(v))) for i in range(len(M))]

def predict_trajectory(x0, U):
    """Predict state and ZMP trajectory given control sequence."""
    states = [x0[:]]
    zmps = []
    x = x0[:]
    for u in U:
        zmp = sum(C[j]*x[j] for j in range(3))
        zmps.append(zmp)
        x_new = [sum(A[i][j]*x[j] for j in range(3)) + B[i]*u for i in range(3)]
        states.append(x_new[:])
        x = x_new
    zmps.append(sum(C[j]*x[j] for j in range(3)))
    return states, zmps

# ZMP reference (step pattern in x-direction)
step_duration = 0.6
step_length = 0.15

def get_zmp_ref(t_start, N_steps):
    refs = []
    for i in range(N_steps):
        t = t_start + i * T
        phase = int(t / step_duration)
        refs.append(phase * step_length)
    return refs

# Initial state
x = [0.0, 0.3, 0.0]  # start with forward velocity

# MPC weights
Q_zmp = 1.0    # ZMP tracking weight
R_u = 1e-6     # control effort weight

print("=== Linear MPC Walking ===")
print(f"Horizon: {N} steps ({N*T:.2f}s)")
print(f"Step length: {step_length}m, Step duration: {step_duration}s")
print()

# Simulate for 2 seconds
sim_steps = int(2.0 / T)
print("Time(s)  CoM_x(m)  ZMP(m)   ZMP_ref(m)  Jerk")
print("-" * 55)

for k in range(sim_steps):
    t = k * T
    zmp_ref = get_zmp_ref(t, N + 1)

    # Simple MPC: solve with gradient descent (simplified)
    U = [0.0] * N
    learning_rate = 0.1

    for opt_iter in range(30):
        states, zmps = predict_trajectory(x, U)
        # Gradient of cost w.r.t. U
        grad = [0.0] * N
        for i in range(N):
            zmp_error = zmps[i] - zmp_ref[i]
            grad[i] = 2 * Q_zmp * zmp_error + 2 * R_u * U[i]
        # Update
        for i in range(N):
            U[i] -= learning_rate * grad[i]
            U[i] = max(-50, min(50, U[i]))  # clamp jerk

    # Apply first control input
    u_applied = U[0]
    zmp = sum(C[j]*x[j] for j in range(3))
    phase = int(t / step_duration)
    zmp_ref_now = phase * step_length

    if k % 5 == 0:
        print(f"{t:5.2f}    {x[0]:7.4f}  {zmp:7.4f}   {zmp_ref_now:7.4f}    {u_applied:7.2f}")

    # State update
    x = [sum(A[i][j]*x[j] for j in range(3)) + B[i]*u_applied for i in range(3)]
`}
/>

## Handling Constraints

A key advantage of MPC over preview control is explicit **constraint handling**:

<MPCConstraintDiagram />

### ZMP Constraints

$$
p_{x,min} \leq C\mathbf{x}_i \leq p_{x,max}
$$

The bounds change with footstep timing (support polygon). These constraints
are expressed as linear inequalities on $\mathbf{U}$ via the prediction matrices:

$$
\bar{C}\Gamma \mathbf{U} \leq \mathbf{b}_{max} - \bar{C}\Phi \mathbf{x}_0, \quad
-\bar{C}\Gamma \mathbf{U} \leq -\mathbf{b}_{min} + \bar{C}\Phi \mathbf{x}_0
$$

### Kinematic Constraints

- Maximum step length
- Foot clearance height
- Joint angle limits

### Timing Adaptation

MPC can optimize **when** to take steps, not just the CoM trajectory.
This enables reactive stepping in response to pushes.

## Nonlinear MPC

For more accurate walking, nonlinear MPC uses the full robot dynamics:

$$
\min_{\mathbf{u}(\cdot)} \int_0^T L(\mathbf{x}, \mathbf{u}) dt + V_f(\mathbf{x}(T))
$$

subject to:
- Full nonlinear dynamics: $\dot{\mathbf{x}} = f(\mathbf{x}, \mathbf{u})$
- Contact complementarity: $\lambda \geq 0, \phi(\mathbf{q}) \geq 0, \lambda \phi = 0$

Nonlinear MPC is computationally expensive but handles:
- Variable CoM height
- Angular momentum
- Multi-contact scenarios

## Real-Time Considerations

Walking MPC must solve within the control loop (~1-10 ms). The following
strategies make this possible:

1. **Warm starting**: Use the previous solution shifted by one step as the initial
   guess for the current QP. Since the problem changes only slightly between
   consecutive time steps, the shifted solution is already close to optimal.
   This dramatically reduces the number of iterations the solver needs.

2. **Early termination**: Accept a suboptimal solution after a fixed number of
   iterations. For walking, a slightly suboptimal control input applied on time
   is far better than the mathematically optimal input that arrives too late.

3. **Condensed formulation**: Substitute the dynamics constraints to eliminate
   all state variables, reducing the QP to a smaller problem in $\mathbf{U}$ only.
   This is exactly the $H$, $\mathbf{f}$ formulation derived above. The trade-off
   is a dense (but smaller) QP versus a sparse (but larger) one.

4. **Tailored solvers**: Walking QPs have banded structure because each state
   depends only on the previous state. Solvers that exploit this sparsity
   (e.g., HPIPM) achieve $O(N)$ complexity per iteration instead of $O(N^3)$,
   making long horizons tractable.

## References

- P.-B. Wieber, "[Trajectory Free Linear Model Predictive Control for Stable Walking in the Presence of Strong Perturbations](https://doi.org/10.1109/ICHR.2006.321375)," *Proc. IEEE-RAS Humanoids*, 2006.
- A. Herdt et al., "[Online Walking Motion Generation with Automatic Footstep Placement](https://doi.org/10.1163/016918610X12681568462IO)," *Advanced Robotics*, 2010.

<InteractiveDemo title="MPC Horizon Visualization">
  <p className="text-sm text-gray-500">
    Interactive MPC with adjustable horizon and constraints coming soon.
  </p>
</InteractiveDemo>
