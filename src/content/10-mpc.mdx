import { MathBlock } from "@/components/math/MathBlock";
import { CodeEditor } from "@/components/code/CodeEditor";
import { InteractiveDemo } from "@/components/visualization/InteractiveDemo";
import { MPCHorizonDiagram } from "@/components/diagrams/MPCDiagram";
import { MPCConstraintDiagram } from "@/components/diagrams/MPCConstraintDiagram";

# Model Predictive Control (MPC)

Model Predictive Control optimizes control inputs over a future horizon at
each time step, providing a powerful framework for walking that handles
constraints explicitly.

## MPC Formulation

At each time step $k$, solve the optimization:

<MathBlock tex="\min_{u_0, \ldots, u_{N-1}} \sum_{i=0}^{N-1} \left[ \|\mathbf{x}_i - \mathbf{x}_i^{ref}\|^2_Q + \|u_i\|^2_R \right] + \|\mathbf{x}_N - \mathbf{x}_N^{ref}\|^2_{Q_f}" />

subject to:
- Dynamics: $\mathbf{x}_{i+1} = A\mathbf{x}_i + Bu_i$
- State constraints: $\mathbf{x}_i \in \mathcal{X}$
- Input constraints: $u_i \in \mathcal{U}$
- ZMP constraint: $p_i \in \text{support polygon}$

Apply only the first input $u_0^*$, then re-solve at the next step.

<MPCHorizonDiagram />

### Why Receding Horizon?

Why not solve one long-horizon problem at the start and execute the entire plan open-loop?
There are three key reasons:

1. **Model mismatch**: The LIPM is a simplification. The real robot has flexible joints,
   unmodeled friction, and other dynamics the model ignores. Small errors accumulate
   over time.
2. **Disturbances**: External pushes, uneven terrain, or slippery surfaces cannot be
   predicted in advance. Re-solving incorporates the measured state as feedback.
3. **Changing objectives**: The desired footstep plan may change mid-walk (e.g., an
   obstacle appears, or a human gives a new goal). MPC naturally adapts because it
   replans from the current state.

By re-solving at each time step with the latest measured state, MPC continuously
corrects for all of these issues. Think of it like a GPS that recalculates your
route every few seconds, rather than computing it once at the start of your trip.

## Linear MPC with LIPM

Using the LIPM state-space model from Chapter 5, the MPC becomes a
**Quadratic Program** (QP) that can be solved efficiently.

### State-Space Model

$$
\mathbf{x}_{k+1} = A\mathbf{x}_k + Bu_k, \quad p_k = C\mathbf{x}_k
$$

with $\mathbf{x} = [x, \dot{x}, \ddot{x}]^T$, $u = \dddot{x}$ (jerk),
and $p$ the ZMP position.

### QP Formulation

The MPC problem can be written as:

$$
\min_{\mathbf{U}} \frac{1}{2}\mathbf{U}^T H \mathbf{U} + \mathbf{f}^T \mathbf{U}
$$

subject to $A_{ineq}\mathbf{U} \leq \mathbf{b}_{ineq}$

where $\mathbf{U} = [u_0, u_1, \ldots, u_{N-1}]^T$ is the stacked control input.

### Constructing the QP from State-Space Model

To obtain the matrices $H$ and $\mathbf{f}$ concretely, we "unroll" the dynamics
over the prediction horizon of $N$ steps.

**Step 1: Stack the dynamics.**
Starting from $\mathbf{x}_0$, we can write all future states as a function of
the initial state and the control sequence:

$$
\mathbf{X} = \Phi \mathbf{x}_0 + \Gamma \mathbf{U}
$$

where $\mathbf{X} = [\mathbf{x}_1, \mathbf{x}_2, \ldots, \mathbf{x}_N]^T$ is the
stacked state vector, and

$$
\Phi = \begin{bmatrix} A \\ A^2 \\ \vdots \\ A^N \end{bmatrix}, \quad
\Gamma = \begin{bmatrix}
B & 0 & \cdots & 0 \\
AB & B & \cdots & 0 \\
\vdots & \vdots & \ddots & \vdots \\
A^{N-1}B & A^{N-2}B & \cdots & B
\end{bmatrix}
$$

$\Phi$ propagates the initial state forward, and $\Gamma$ is a lower-triangular
matrix that maps control inputs to future states.

**Step 2: Predict ZMP trajectory.**
The ZMP at each step is $p_i = C \mathbf{x}_i$. Stacking all ZMP predictions:

$$
\mathbf{P} = \bar{C} \mathbf{X} = \bar{C} \Phi \mathbf{x}_0 + \bar{C} \Gamma \mathbf{U}
$$

where $\bar{C} = \text{diag}(C, C, \ldots, C)$ is a block-diagonal matrix.

**Step 3: Form the cost function.**
The ZMP tracking cost is:

$$
J = (\mathbf{P} - \mathbf{P}_{ref})^T Q (\mathbf{P} - \mathbf{P}_{ref}) + \mathbf{U}^T R \mathbf{U}
$$

Expanding this and collecting terms in $\mathbf{U}$:

$$
H = \Gamma^T \bar{C}^T Q \bar{C} \Gamma + R
$$

$$
\mathbf{f} = \Gamma^T \bar{C}^T Q (\bar{C} \Phi \mathbf{x}_0 - \mathbf{P}_{ref})
$$

**Small example (N=3):** With a sampling period $T = 0.1$ s, each column
of $\Gamma$ contains the influence of one control input on all future states.
For instance, $u_0$ affects $\mathbf{x}_1$ through $B$, $\mathbf{x}_2$ through
$AB$, and $\mathbf{x}_3$ through $A^2B$. Meanwhile $u_2$ only affects
$\mathbf{x}_3$ through $B$. This lower-triangular structure is key:
earlier control inputs have more influence on the predicted trajectory than
later ones.

## MPC vs. LQR vs. Preview Control

Understanding how MPC relates to other controllers from earlier chapters
clarifies when to use each method.

- **LQR** (Chapter 4) is a special case: it is equivalent to infinite-horizon MPC
  with no constraints. LQR produces a fixed gain matrix $K$ computed offline,
  so there is no online optimization.
- **Preview control** (Chapter 5) is another special case: infinite-horizon with a
  preview window of future references, but still no inequality constraints.
- **MPC's key advantage is constraints.** It can explicitly enforce that the ZMP stays
  inside the support polygon, that step lengths are bounded, etc.
- When no constraints are active, MPC and LQR produce nearly identical control
  inputs. The constraint handling is what makes MPC worth the extra computation.

| Feature | Preview Control | MPC |
|---|---|---|
| Constraints | No | Yes |
| Computation | Offline gain matrices | Online QP at each step |
| Optimality | Infinite horizon | Finite horizon (N steps) |
| Reactivity | Limited (fixed gains) | High (replanning each step) |
| Footstep modification | Cannot change online | Can adapt footsteps online |

<CodeEditor
  initialCode={`import math
import matplotlib
matplotlib.use('agg')
import matplotlib.pyplot as plt
import numpy as np
import io, base64

# Simple Linear MPC for LIPM walking
# Using unconstrained gradient descent (for demonstration only).
#
# Note: This simplified example uses unconstrained gradient descent
# for illustration. In practice, walking MPC uses dedicated QP solvers
# (qpOASES, OSQP, Clarabel) that handle inequality constraints
# efficiently. Typical computation times are 0.1-5 ms per solve,
# allowing control at 100-1000 Hz.

g = 9.81
z_c = 0.8
T = 0.02  # sampling period
N = 40    # prediction horizon

# LIPM matrices
A = [[1, T, T**2/2], [0, 1, T], [0, 0, 1]]
B = [T**3/6, T**2/2, T]
C = [1, 0, -z_c/g]

step_duration = 0.6
step_length = 0.15

def predict_trajectory(x0, U):
    """Predict state and ZMP trajectory given control sequence."""
    states = [x0[:]]
    zmps = []
    x = x0[:]
    for u in U:
        zmp = sum(C[j]*x[j] for j in range(3))
        zmps.append(zmp)
        x_new = [
            sum(A[i][j]*x[j] for j in range(3)) + B[i]*u
            for i in range(3)
        ]
        states.append(x_new[:])
        x = x_new
    zmps.append(sum(C[j]*x[j] for j in range(3)))
    return states, zmps

def get_zmp_ref(t_start, n_steps):
    """Generate ZMP reference based on step pattern."""
    refs = []
    for i in range(n_steps):
        t = t_start + i * T
        phase = int(t / step_duration)
        refs.append(phase * step_length)
    return refs

def compute_support_bounds(t):
    """Return support polygon bounds at time t."""
    phase = int(t / step_duration)
    center = phase * step_length
    half_foot = 0.07
    return center - half_foot, center + half_foot

# Initial state
x = [0.0, 0.3, 0.0]
Q_zmp = 1.0
R_u = 1e-6

print("=== Linear MPC Walking ===")
print("Horizon: {} steps ({:.2f}s)".format(N, N * T))
print("Step length: {}m, Step duration: {}s".format(
    step_length, step_duration))
print()

sim_steps = int(2.0 / T)
print("Time(s)  CoM_x(m)  ZMP(m)   ZMP_ref(m)  Jerk")
print("-" * 55)

# Storage for plotting
time_log = []
com_log = []
zmp_log = []
ref_log = []
jerk_log = []
sp_lo_log = []
sp_hi_log = []

for k in range(sim_steps):
    t = k * T
    zmp_ref = get_zmp_ref(t, N + 1)

    U = [0.0] * N
    lr = 0.1
    for _ in range(30):
        _, zmps = predict_trajectory(x, U)
        grad = [0.0] * N
        for i in range(N):
            err = zmps[i] - zmp_ref[i]
            grad[i] = 2 * Q_zmp * err + 2 * R_u * U[i]
        for i in range(N):
            U[i] -= lr * grad[i]
            U[i] = max(-50, min(50, U[i]))

    u_applied = U[0]
    zmp = sum(C[j] * x[j] for j in range(3))
    zmp_ref_now = int(t / step_duration) * step_length
    lo, hi = compute_support_bounds(t)

    time_log.append(t)
    com_log.append(x[0])
    zmp_log.append(zmp)
    ref_log.append(zmp_ref_now)
    jerk_log.append(u_applied)
    sp_lo_log.append(lo)
    sp_hi_log.append(hi)

    if k % 5 == 0:
        print("{:5.2f}    {:7.4f}  {:7.4f}   {:7.4f}    {:7.2f}".format(
            t, x[0], zmp, zmp_ref_now, u_applied))

    x = [
        sum(A[i][j]*x[j] for j in range(3)) + B[i]*u_applied
        for i in range(3)
    ]

# --- Plot results ---
t_arr = np.array(time_log)
fig, axes = plt.subplots(2, 1, figsize=(8, 6), sharex=True)

ax0 = axes[0]
ax0.fill_between(t_arr, sp_lo_log, sp_hi_log,
                 alpha=0.15, color='gray', label='Support polygon')
ax0.plot(t_arr, com_log, 'b-', linewidth=1.2, label='CoM_x')
ax0.plot(t_arr, zmp_log, 'g-', linewidth=0.8, label='ZMP')
ax0.plot(t_arr, ref_log, 'r--', linewidth=1.0, label='ZMP_ref')
ax0.set_ylabel('Position (m)')
ax0.set_title('Linear MPC Walking: CoM and ZMP Trajectories')
ax0.legend(loc='upper left', fontsize=8)
ax0.grid(True, alpha=0.3)

ax1 = axes[1]
ax1.plot(t_arr, jerk_log, 'm-', linewidth=0.8)
ax1.set_xlabel('Time (s)')
ax1.set_ylabel('Jerk input u (m/s3)')
ax1.set_title('Control Input (Jerk)')
ax1.grid(True, alpha=0.3)
ax1.axhline(y=0, color='k', linewidth=0.5)

plt.tight_layout()
buf = io.BytesIO()
plt.savefig(buf, format='png', dpi=100, bbox_inches='tight')
buf.seek(0)
img = base64.b64encode(buf.read()).decode('utf-8')
print('data:image/png;base64,' + img)
plt.close()`}
/>

## Simulation: Effect of Prediction Horizon

The prediction horizon $N$ is one of the most important MPC parameters.
A longer horizon lets the controller anticipate future reference changes
(e.g., upcoming footsteps) but increases computation. Below we compare
$N=5$, $N=15$, and $N=30$ on the same walking scenario.

<CodeEditor
  initialCode={`import math
import matplotlib
matplotlib.use('agg')
import matplotlib.pyplot as plt
import numpy as np
import io, base64

g = 9.81
z_c = 0.8
T = 0.02
step_duration = 0.6
step_length = 0.15

A = [[1, T, T**2/2], [0, 1, T], [0, 0, 1]]
B = [T**3/6, T**2/2, T]
C = [1, 0, -z_c/g]

def predict_trajectory(x0, U):
    """Predict ZMP trajectory for a given control sequence."""
    zmps = []
    x = x0[:]
    for u in U:
        zmp = sum(C[j]*x[j] for j in range(3))
        zmps.append(zmp)
        x = [
            sum(A[i][j]*x[j] for j in range(3)) + B[i]*u
            for i in range(3)
        ]
    zmps.append(sum(C[j]*x[j] for j in range(3)))
    return zmps

def get_zmp_ref(t_start, n_steps):
    """Generate ZMP reference for the prediction window."""
    refs = []
    for i in range(n_steps):
        t = t_start + i * T
        refs.append(int(t / step_duration) * step_length)
    return refs

def run_mpc_sim(horizon_n):
    """Run a full MPC simulation with a given prediction horizon."""
    x = [0.0, 0.3, 0.0]
    sim_steps = int(2.0 / T)
    Q_zmp = 1.0
    R_u = 1e-6
    lr = 0.1

    t_log, com_log, zmp_log, ref_log = [], [], [], []

    for k in range(sim_steps):
        t = k * T
        zmp_ref = get_zmp_ref(t, horizon_n + 1)

        U = [0.0] * horizon_n
        for _ in range(30):
            zmps = predict_trajectory(x, U)
            grad = [0.0] * horizon_n
            for i in range(horizon_n):
                err = zmps[i] - zmp_ref[i]
                grad[i] = 2*Q_zmp*err + 2*R_u*U[i]
            for i in range(horizon_n):
                U[i] -= lr * grad[i]
                U[i] = max(-50, min(50, U[i]))

        u_applied = U[0]
        zmp = sum(C[j]*x[j] for j in range(3))
        ref_now = int(t / step_duration) * step_length

        t_log.append(t)
        com_log.append(x[0])
        zmp_log.append(zmp)
        ref_log.append(ref_now)

        x = [
            sum(A[i][j]*x[j] for j in range(3)) + B[i]*u_applied
            for i in range(3)
        ]
    return t_log, com_log, zmp_log, ref_log

horizons = [5, 15, 30]
results = {}
for nh in horizons:
    results[nh] = run_mpc_sim(nh)

fig, axes = plt.subplots(1, 3, figsize=(12, 4), sharey=True)

for idx, nh in enumerate(horizons):
    t_arr, com_arr, zmp_arr, ref_arr = results[nh]
    t_np = np.array(t_arr)
    zmp_np = np.array(zmp_arr)
    ref_np = np.array(ref_arr)

    tracking_rmse = float(np.sqrt(np.mean((zmp_np - ref_np)**2)))

    ax = axes[idx]
    ax.plot(t_np, com_arr, 'b-', linewidth=1.0, label='CoM_x')
    ax.plot(t_np, zmp_arr, 'g-', linewidth=0.7, label='ZMP')
    ax.plot(t_np, ref_arr, 'r--', linewidth=0.8, label='ZMP_ref')
    ax.set_xlabel('Time (s)')
    if idx == 0:
        ax.set_ylabel('Position (m)')
    ax.set_title('N = {}'.format(nh))
    ax.legend(fontsize=7, loc='upper left')
    ax.grid(True, alpha=0.3)
    ax.annotate('RMSE = {:.4f} m'.format(tracking_rmse),
                xy=(0.98, 0.02), xycoords='axes fraction',
                ha='right', va='bottom', fontsize=8,
                bbox=dict(boxstyle='round,pad=0.3',
                          fc='wheat', alpha=0.7))

fig.suptitle('Effect of Prediction Horizon on MPC Tracking',
             fontsize=13)
plt.tight_layout()
buf = io.BytesIO()
plt.savefig(buf, format='png', dpi=100, bbox_inches='tight')
buf.seek(0)
img = base64.b64encode(buf.read()).decode('utf-8')
print('data:image/png;base64,' + img)
plt.close()

print()
print("=== Horizon Comparison Summary ===")
for nh in horizons:
    _, _, zmp_arr, ref_arr = results[nh]
    zmp_np = np.array(zmp_arr)
    ref_np = np.array(ref_arr)
    rmse = float(np.sqrt(np.mean((zmp_np - ref_np)**2)))
    print("N={:2d}  horizon={:.2f}s  ZMP RMSE={:.5f} m".format(
        nh, nh*T, rmse))`}
/>

With a short horizon ($N=5$, only 0.10 s lookahead), the controller cannot
anticipate the next footstep transition and reacts late, producing large
tracking errors at each step change. A moderate horizon ($N=15$, 0.30 s)
provides enough lookahead to begin transitioning the CoM before the step
change, improving tracking significantly. A long horizon ($N=30$, 0.60 s)
sees one full step ahead and achieves the best tracking, though with
diminishing returns beyond a certain point.

## Handling Constraints

A key advantage of MPC over preview control is explicit **constraint handling**:

<MPCConstraintDiagram />

### ZMP Constraints

$$
p_{x,min} \leq C\mathbf{x}_i \leq p_{x,max}
$$

The bounds change with footstep timing (support polygon). These constraints
are expressed as linear inequalities on $\mathbf{U}$ via the prediction matrices:

$$
\bar{C}\Gamma \mathbf{U} \leq \mathbf{b}_{max} - \bar{C}\Phi \mathbf{x}_0, \quad
-\bar{C}\Gamma \mathbf{U} \leq -\mathbf{b}_{min} + \bar{C}\Phi \mathbf{x}_0
$$

### Kinematic Constraints

- Maximum step length
- Foot clearance height
- Joint angle limits

### Timing Adaptation

MPC can optimize **when** to take steps, not just the CoM trajectory.
This enables reactive stepping in response to pushes.

## Nonlinear MPC

For more accurate walking, nonlinear MPC uses the full robot dynamics:

$$
\min_{\mathbf{u}(\cdot)} \int_0^T L(\mathbf{x}, \mathbf{u}) dt + V_f(\mathbf{x}(T))
$$

subject to:
- Full nonlinear dynamics: $\dot{\mathbf{x}} = f(\mathbf{x}, \mathbf{u})$
- Contact complementarity: $\lambda \geq 0, \phi(\mathbf{q}) \geq 0, \lambda \phi = 0$

Nonlinear MPC is computationally expensive but handles:
- Variable CoM height
- Angular momentum
- Multi-contact scenarios

## Real-Time Considerations

Walking MPC must solve within the control loop (~1-10 ms). The following
strategies make this possible:

1. **Warm starting**: Use the previous solution shifted by one step as the initial
   guess for the current QP. Since the problem changes only slightly between
   consecutive time steps, the shifted solution is already close to optimal.
   This dramatically reduces the number of iterations the solver needs.

2. **Early termination**: Accept a suboptimal solution after a fixed number of
   iterations. For walking, a slightly suboptimal control input applied on time
   is far better than the mathematically optimal input that arrives too late.

3. **Condensed formulation**: Substitute the dynamics constraints to eliminate
   all state variables, reducing the QP to a smaller problem in $\mathbf{U}$ only.
   This is exactly the $H$, $\mathbf{f}$ formulation derived above. The trade-off
   is a dense (but smaller) QP versus a sparse (but larger) one.

4. **Tailored solvers**: Walking QPs have banded structure because each state
   depends only on the previous state. Solvers that exploit this sparsity
   (e.g., HPIPM) achieve $O(N)$ complexity per iteration instead of $O(N^3)$,
   making long horizons tractable.

## References

- P.-B. Wieber, "[Trajectory Free Linear Model Predictive Control for Stable Walking in the Presence of Strong Perturbations](https://doi.org/10.1109/ICHR.2006.321375)," *Proc. IEEE-RAS Humanoids*, 2006.
- A. Herdt et al., "[Online Walking Motion Generation with Automatic Footstep Placement](https://doi.org/10.1163/016918610X12681568462IO)," *Advanced Robotics*, 2010.

<InteractiveDemo title="MPC Horizon Visualization">
  <p className="text-sm text-gray-500">
    Interactive MPC with adjustable horizon and constraints coming soon.
  </p>
</InteractiveDemo>
