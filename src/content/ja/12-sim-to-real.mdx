import { MathBlock } from "@/components/math/MathBlock";
import { CodeEditor } from "@/components/code/CodeEditor";
import { InteractiveDemo } from "@/components/visualization/InteractiveDemo";
import { SimToRealPipelineDiagram } from "@/components/diagrams/SimToRealDiagram";
import { TeacherStudentDiagram } from "@/components/diagrams/TeacherStudentDiagram";

# シミュレーションから実機への転移

シミュレーション内での歩行方策の学習は高速かつ安全ですが、実際のロボットへの転移は**シミュレーションと実機のギャップ**（シミュレーションと実世界の物理の違い）のため困難です。

<SimToRealPipelineDiagram />

## シミュレーションと実機のギャップ

シミュレーションと現実の間の不一致の原因:

| 原因 | 例 |
|--------|---------|
| **ダイナミクス** | 不正確な質量、摩擦、接触モデル |
| **アクチュエータ** | モータ遅延、トルク制限、バックラッシュ |
| **センサ** | ノイズ、バイアス、キャリブレーション誤差 |
| **環境** | 地形の変化、風、障害物 |

## ドメインランダム化

**ドメインランダム化**は、シミュレーションパラメータの広い分布にわたって方策を学習させ、変動に対してロバストにします。

### 何をランダム化するか

$$
\xi \sim \mathcal{U}(\xi_{min}, \xi_{max})
$$

| パラメータ | 典型的な範囲 |
|-----------|--------------|
| 質量 | 公称値の $\pm 30\%$ |
| 摩擦 | $[0.3, 1.5]$ |
| モータ強度 | $\pm 20\%$ |
| センサノイズ | $\sigma \in [0, 0.05]$ |
| 動作遅延 | $[0, 30]$ ms |
| 地面の傾斜 | $\pm 5°$ |

<CodeEditor
  initialCode={`import random
import math

# Domain randomization demonstration
random.seed(42)

class RandomizedEnvironment:
    """LIPM with randomized parameters."""
    def __init__(self):
        self.randomize()

    def randomize(self):
        """Randomize physics parameters."""
        nominal_mass = 30.0
        nominal_height = 0.8
        nominal_friction = 0.8

        self.mass = nominal_mass * random.uniform(0.7, 1.3)
        self.z_c = nominal_height * random.uniform(0.85, 1.15)
        self.friction = nominal_friction * random.uniform(0.5, 1.5)
        self.motor_delay = random.uniform(0, 0.03)
        self.sensor_noise = random.uniform(0, 0.02)
        self.g = 9.81

        self.omega = math.sqrt(self.g / self.z_c)

    def get_params_str(self):
        return (f"mass={self.mass:.1f}kg, z_c={self.z_c:.3f}m, "
                f"friction={self.friction:.2f}, delay={self.motor_delay*1000:.0f}ms")

# Show parameter distributions across environments
print("=== Domain Randomization: Parameter Samples ===")
print()
print("Env  Mass(kg)  Height(m)  Friction  Delay(ms)  omega(rad/s)")
print("-" * 65)

env = RandomizedEnvironment()
omega_values = []

for i in range(10):
    env.randomize()
    omega_values.append(env.omega)
    print(f" {i:2d}   {env.mass:6.1f}     {env.z_c:.3f}     "
          f"{env.friction:.2f}      {env.motor_delay*1000:4.1f}       "
          f"{env.omega:.3f}")

print()
avg_omega = sum(omega_values) / len(omega_values)
std_omega = math.sqrt(sum((w - avg_omega)**2 for w in omega_values) / len(omega_values))
print(f"omega range: [{min(omega_values):.3f}, {max(omega_values):.3f}]")
print(f"omega mean: {avg_omega:.3f}, std: {std_omega:.3f}")
print()

# Simulate effect on walking
print("=== Effect on Walking Behavior ===")
print()
print("With a fixed controller, different environments produce:")
print()

for i in range(5):
    env.randomize()
    # Simple walking step with LIPM
    x = -0.05
    xdot = 0.3
    dt = 0.01
    step_time = 0.5

    for _ in range(int(step_time / dt)):
        xddot = env.omega**2 * x  # simplified
        xdot += xddot * dt
        x += xdot * dt

    print(f"  Env {i}: omega={env.omega:.3f} -> final_x={x:.4f}m, "
          f"vel={xdot:.4f}m/s")

print()
print("A robust policy must handle all these variations!")
`}
/>

## ドメインランダム化が機能する理由

パラメータをランダム化するとなぜロバストな方策が得られるのでしょうか。重要な洞察は、方策が単一の環境に対してではなく、環境の**分布全体にわたる期待性能**を最適化するようになることです:

$$
\pi^* = \arg\max_\pi \; \mathbb{E}_{\xi \sim P(\xi)}[J(\pi, \xi)]
$$

ここで $J(\pi, \xi)$ はパラメータ $\xi$ の環境における方策 $\pi$ のリターン（累積報酬）、$P(\xi)$ はランダム化分布です。

多くの異なる環境にわたる平均性能を最大化することで、方策は特定のシミュレーション設定の癖を利用するのではなく、**一般的に有効な戦略**を発見せざるを得なくなります。車の運転に例えると、晴れた乾いた道路でしか練習しなければ、雨や雪で失敗するかもしれません。雨、雪、晴れのすべてで練習すれば、あらゆる条件にそこそこ対応できるドライバーになれます。

## 失敗モードとトレードオフ

ドメインランダム化には慎重な調整が必要です。主に二つの失敗モードがあります:

- **過度に保守的な行動**: ランダム化の範囲が広すぎると、方策は極端なシナリオに備えなければなりません。その結果、パラメータ空間の隅で失敗する可能性のある積極的な戦略を取れず、遅く慎重すぎる歩行になることが多いです。

- **ランダム化不足**: 範囲が狭すぎると、方策はシミュレーション固有の特徴（例：完全に平坦な地面、センサ遅延ゼロ）に過適合する可能性があります。これらは実ロボットには存在しないため、学習分布外の実環境条件に遭遇すると失敗します。

**ランダム化範囲設定の実践的ガイドライン**:

1. **物理的公差から始める**: 質量範囲を製造公差内（例：$\pm 5\%$）に、摩擦を測定された床面範囲内に設定します。
2. **不確実なパラメータはより広くランダム化する**: 正確に測定しにくいパラメータ（摩擦係数、減衰比、接触剛性）は、よく特性化されたパラメータ（リンク質量、幾何形状）よりも広い範囲にすべきです。
3. **転移結果に基づいて反復する**: 実ロボットに展開し、失敗を観察し、関連するランダム化範囲を拡大します。

## システム同定

すべてをランダム化する代わりに、**システム同定**は実際のロボットのパラメータを測定してシミュレーション精度を向上させます。

### オンラインシステム同定

展開中にパラメータを推定します:

$$
\hat{\xi}_{k+1} = \hat{\xi}_k + K_k(y_k - \hat{y}_k(\hat{\xi}_k))
$$

ここで $\hat{\xi}$ はパラメータ推定値、$y_k$ は計測出力、$K_k$ は適応ゲインです。

## 教師-生徒学習

Sim-to-Real転移で広く使われるアプローチが**教師-生徒蒸留**です。この考え方は、方策が*知る*必要があることと、実ロボットで実際に*観測*できることを分離することです。

<TeacherStudentDiagram />

**フェーズ1（教師の学習）**: 教師ネットワークはシミュレーション内で**特権情報**にアクセスして学習します。これは摩擦、質量、地形形状、接触力などの真値で、シミュレータ内では利用可能ですが実ロボットでは測定不可能な情報です。教師はすべてを見られるため、ほぼ最適な方策を学習できます。

**フェーズ2（生徒の蒸留）**: 生徒ネットワークは**実ロボットで利用可能な観測**（関節エンコーダ、IMU、指令動作）のみを受け取ります。教師の行動を模倣するように教師あり学習で訓練されます。損失関数は単純に:

$$
\mathcal{L} = \mathbb{E}\left[\|\pi_S(s) - \pi_T(s, \text{priv})\|^2\right]
$$

生徒は、観測可能な入力のパターンから欠落した特権情報を暗黙的に推論することを学習します。例えば、摩擦が低いとロボットはより滑りやすくなり、生徒はその滑りパターンを認識して教師のように対応することを学びます。

## 高速モータ適応 (RMA)

RMAはドメインランダム化とオンライン適応の利点を組み合わせます:

1. **基本方策** $\pi(a | s, z)$: 状態と環境埋め込み $z$ を入力として受け取る
2. **適応モジュール** $\hat{z} = f(s_{t-H:t})$: 最近の状態履歴から $z$ を推定

学習時（シミュレーション）:
- 特権的な環境情報 $z$ を用いて基本方策を学習
- 状態履歴から $z$ を予測する適応モジュールを学習

展開時（実ロボット）:
- 適応モジュールがセンサデータからオンラインで $z$ を推定
- 基本方策が推定された $z$ を使用して行動

### RMAの詳細な仕組み

適応モジュールは**最近の観測の履歴**を入力として受け取ります。典型的には直近 $H = 50$ タイムステップ分の関節位置、速度、指令動作です。この時間窓から、現在の環境パラメータ（摩擦、質量分布、地形の傾斜）を暗黙的に推定します。

出力は**環境エンコーディング**ベクトル $z \in \mathbb{R}^d$（典型的には $d = 8$ から $32$）で、現在の観測 $s_t$ と結合されて基本方策に入力されます:

$$
a_t = \pi(s_t, \hat{z}_t), \quad \hat{z}_t = f_\phi(s_{t-H}, \ldots, s_t)
$$

適応モジュール $f_\phi$ は、教師が真のパラメータから計算する特権環境エンコーディング $z^*$ との回帰損失で学習されます:

$$
\mathcal{L}_\text{adapt} = \mathbb{E}\left[\|\hat{z}_t - z^*\|^2\right]
$$

**なぜ履歴に環境情報が含まれるのでしょうか？** それは、同じ指令動作に対するロボットの応答が物理パラメータによって異なるからです。滑りやすい床では蹴り出し時に足が滑り、重い荷物を積んでいれば同じトルクでも加速度が小さくなります。適応モジュールはこれらの特徴を観測履歴から読み取ることを学習します。

<CodeEditor
  initialCode={`import math
import random

# Simplified Rapid Motor Adaptation concept
random.seed(42)

# Environment parameters (the "latent" z)
true_params = {
    "mass_ratio": 1.2,     # 20% heavier than nominal
    "friction": 0.6,       # lower friction
    "motor_strength": 0.9,  # 10% weaker motors
}

# Simulate state history with these parameters
def simulate_step(x, v, force, mass_ratio, friction):
    dt = 0.01
    a = force / (30.0 * mass_ratio) - friction * v * 0.1
    v_new = v + a * dt
    x_new = x + v_new * dt
    return x_new, v_new

# Generate state history
print("=== Rapid Motor Adaptation Demo ===")
print()
print("True environment parameters:")
for k, v in true_params.items():
    print(f"  {k}: {v}")
print()

history = []
x, v = 0.0, 0.0
for t in range(50):
    force = 5.0 * math.sin(t * 0.2)  # probing signal
    x, v = simulate_step(x, v, force,
                          true_params["mass_ratio"],
                          true_params["friction"])
    history.append((x, v, force))

# "Adaptation module": estimate parameters from history
# (In practice, this is a neural network)
# Simple estimation: use response characteristics
responses = [h[1] for h in history[10:]]  # velocities
forces = [h[2] for h in history[10:]]

# Estimate mass ratio from force-acceleration relationship
accels = [(responses[i+1] - responses[i]) / 0.01 for i in range(len(responses)-1)]
avg_response = sum(abs(a) for a in accels) / len(accels)
nominal_response = 0.17  # expected for nominal params
estimated_mass_ratio = nominal_response / avg_response * 1.0

# Estimate friction from velocity decay
vel_decay = sum(abs(responses[i+1]) - abs(responses[i])
                for i in range(len(responses)-1) if abs(forces[i]) < 0.1)

print("Adaptation module estimates:")
print(f"  mass_ratio: {estimated_mass_ratio:.2f} (true: {true_params['mass_ratio']:.2f})")
print()
print("With adaptation, the policy adjusts its behavior:")
print("  - Heavier mass -> stronger push-off")
print("  - Lower friction -> shorter steps")
print("  - Weaker motors -> conservative gait")
`}
/>

## 実践的なデプロイメント

### 接触モデリング

接触モデリングはSim-to-Realギャップの最大の原因です。実際の接触には柔らかい変形可能な材料、複雑な足の形状、表面の不規則性が関与しており、忠実にシミュレートすることは極めて困難です。ほとんどのシミュレータは簡略化された摩擦錐を持つ剛体接触を使用しており、特にヒールストライクやトーオフのフェーズで現実と乖離します。

### アクチュエータモデリング

実際のモータにはシミュレーションで捉えにくい特性があります：熱効果（モータは温度上昇で弱くなる）、非線形摩擦（静止摩擦、クーロン摩擦、粘性減衰）、コントローラとモータドライバ間の通信遅延、速度によって変化する電流/トルク制限などです。これらの効果は動的な歩行中に複合的に作用します。

### センサノイズと遅延

実際のIMUにはジャイロスコープのドリフト、加速度計のバイアス、磁力計の干渉があります。関節エンコーダには量子化ノイズがあります。すべてのセンサ読み取り値には遅延（典型的には1-10 ms）が伴います。シミュレートされたセンサノイズと遅延での学習は不可欠ですが、実際のノイズ特性と正確に一致させることは困難です。

### エンジニアリングの労力

特定のロボットにうまく転移するシミュレーション環境の構築には、通常かなりのエンジニアリング労力が必要で、新しいプラットフォームでは数ヶ月かかることもあります。これにはCADからURDFへの慎重な変換、アクチュエータモデルの調整、センサノイズの特性化、実ロボット実験に基づく反復的な改善が含まれます。

## シミュレーションから実機への主要な成果

| 年 | システム | 手法 | 成果 |
|------|--------|--------|-------------|
| 2018 | Cassie | ドメインランダム化 | 屋外歩行 |
| 2020 | ANYmal | 教師-生徒学習 | ブラインド歩行 |
| 2023 | Digit | RMA | マルチ地形対応 |
| 2024 | 各種 | 大規模RL | アジャイルヒューマノイド歩行 |

## 他の章との関連

- **強化学習（第11章）**: Sim-to-Real転移はほぼ常にRLで学習された方策と組み合わせて使用されます。本章の手法（ドメインランダム化、教師-生徒学習、RMA）は、RL歩行方策を実機に展開するための標準的な手法です。
- **システム同定 vs. ドメインランダム化**: 従来のロボティクスでは正確なモデルを構築するために慎重なシステム同定に依存していました。現代のRLベースのアプローチではドメインランダム化を代わりに使用し、モデルの精度を方策のロバスト性と交換しています。
- **モデル予測制御（第10章）**: MPCベースのアプローチはオンラインでリアルタイムの状態推定で更新可能なモデルを使って再計画するため、Sim-to-Realギャップの多くを回避します。しかし、MPCは学習された方策と比較して、より単純なダイナミクスモデルと短いホライズンに制限されます。

## 参考文献

- J. Tobin et al., "[Domain Randomization for Transferring Deep Neural Networks from Simulation to the Real World](https://arxiv.org/abs/1703.06907)," *Proc. IROS*, 2017.
- I. Radosavovic et al., "[Real-World Humanoid Locomotion with Reinforcement Learning](https://arxiv.org/abs/2303.03381)," *Science Robotics*, 2024.
- A. Kumar et al., "[RMA: Rapid Motor Adaptation for Legged Robots](https://arxiv.org/abs/2107.04034)," *RSS*, 2021.
- J. Lee et al., "[Learning Quadrupedal Locomotion over Challenging Terrain](https://arxiv.org/abs/2010.11251)," *Science Robotics*, 2020.

<InteractiveDemo title="Domain Randomization Visualization">
  <p className="text-sm text-gray-500">
    Interactive domain randomization parameter visualization coming soon.
  </p>
</InteractiveDemo>
