import { MathBlock } from "@/components/math/MathBlock";
import { CodeEditor } from "@/components/code/CodeEditor";
import { InteractiveDemo } from "@/components/visualization/InteractiveDemo";
import { SimToRealPipelineDiagram } from "@/components/diagrams/SimToRealDiagram";
import { TeacherStudentDiagram } from "@/components/diagrams/TeacherStudentDiagram";

# Sim-to-Real Transfer

Training locomotion policies in simulation is fast and safe, but transferring
to real robots is challenging due to the **sim-to-real gap** — differences
between simulated and real-world physics.

<SimToRealPipelineDiagram />

## The Sim-to-Real Gap

Sources of discrepancy between simulation and reality:

| Source | Example |
|--------|---------|
| **Dynamics** | Inaccurate mass, friction, contact models |
| **Actuators** | Motor delays, torque limits, backlash |
| **Sensors** | Noise, bias, calibration errors |
| **Environment** | Terrain variations, wind, obstacles |

## Domain Randomization

**Domain randomization** trains the policy across a wide distribution of
simulation parameters, forcing it to be robust to variations.

### What to Randomize

$$
\xi \sim \mathcal{U}(\xi_{min}, \xi_{max})
$$

| Parameter | Typical Range |
|-----------|--------------|
| Mass | $\pm 30\%$ of nominal |
| Friction | $[0.3, 1.5]$ |
| Motor strength | $\pm 20\%$ |
| Sensor noise | $\sigma \in [0, 0.05]$ |
| Action delay | $[0, 30]$ ms |
| Ground slope | $\pm 5°$ |

<CodeEditor
  initialCode={`import random
import math

# Domain randomization demonstration
random.seed(42)

class RandomizedEnvironment:
    """LIPM with randomized parameters."""
    def __init__(self):
        self.randomize()

    def randomize(self):
        """Randomize physics parameters."""
        nominal_mass = 30.0
        nominal_height = 0.8
        nominal_friction = 0.8

        self.mass = nominal_mass * random.uniform(0.7, 1.3)
        self.z_c = nominal_height * random.uniform(0.85, 1.15)
        self.friction = nominal_friction * random.uniform(0.5, 1.5)
        self.motor_delay = random.uniform(0, 0.03)
        self.sensor_noise = random.uniform(0, 0.02)
        self.g = 9.81

        self.omega = math.sqrt(self.g / self.z_c)

    def get_params_str(self):
        return (f"mass={self.mass:.1f}kg, z_c={self.z_c:.3f}m, "
                f"friction={self.friction:.2f}, delay={self.motor_delay*1000:.0f}ms")

# Show parameter distributions across environments
print("=== Domain Randomization: Parameter Samples ===")
print()
print("Env  Mass(kg)  Height(m)  Friction  Delay(ms)  omega(rad/s)")
print("-" * 65)

env = RandomizedEnvironment()
omega_values = []

for i in range(10):
    env.randomize()
    omega_values.append(env.omega)
    print(f" {i:2d}   {env.mass:6.1f}     {env.z_c:.3f}     "
          f"{env.friction:.2f}      {env.motor_delay*1000:4.1f}       "
          f"{env.omega:.3f}")

print()
avg_omega = sum(omega_values) / len(omega_values)
std_omega = math.sqrt(sum((w - avg_omega)**2 for w in omega_values) / len(omega_values))
print(f"omega range: [{min(omega_values):.3f}, {max(omega_values):.3f}]")
print(f"omega mean: {avg_omega:.3f}, std: {std_omega:.3f}")
print()

# Simulate effect on walking
print("=== Effect on Walking Behavior ===")
print()
print("With a fixed controller, different environments produce:")
print()

for i in range(5):
    env.randomize()
    # Simple walking step with LIPM
    x = -0.05
    xdot = 0.3
    dt = 0.01
    step_time = 0.5

    for _ in range(int(step_time / dt)):
        xddot = env.omega**2 * x  # simplified
        xdot += xddot * dt
        x += xdot * dt

    print(f"  Env {i}: omega={env.omega:.3f} -> final_x={x:.4f}m, "
          f"vel={xdot:.4f}m/s")

print()
print("A robust policy must handle all these variations!")

# --- Visualization of domain randomization ---
import matplotlib
matplotlib.use("agg")
import matplotlib.pyplot as plt
import numpy as np
import io, base64

def plot_domain_randomization(num_envs=50, seed=42):
    """Plot parameter distributions and walking trajectories."""
    rng = np.random.RandomState(seed)

    # Sample randomized parameters
    nominal_mass = 30.0
    nominal_height = 0.8
    nominal_friction = 0.8
    masses = nominal_mass * rng.uniform(0.7, 1.3, num_envs)
    heights = nominal_height * rng.uniform(0.85, 1.15, num_envs)
    frictions = nominal_friction * rng.uniform(0.5, 1.5, num_envs)

    fig, axes = plt.subplots(2, 1, figsize=(8, 7))

    # Top: histograms of randomized parameters
    ax = axes[0]
    ax2 = ax.twinx()
    bins = 12
    ax.hist(masses, bins=bins, alpha=0.6, color="#2563eb",
            label="Mass (kg)")
    ax2.hist(frictions, bins=bins, alpha=0.6, color="#dc2626",
             label="Friction")
    ax.set_xlabel("Parameter Value")
    ax.set_ylabel("Count (Mass)", color="#2563eb")
    ax2.set_ylabel("Count (Friction)", color="#dc2626")
    ax.set_title("Randomized Parameter Distributions")
    lines1, labels1 = ax.get_legend_handles_labels()
    lines2, labels2 = ax2.get_legend_handles_labels()
    ax.legend(lines1 + lines2, labels1 + labels2,
              loc="upper right")
    ax.grid(axis="y", alpha=0.3)

    # Bottom: walking trajectories under different envs
    ax3 = axes[1]
    sim_dt = 0.01
    sim_time = 0.8
    time_steps = int(sim_time / sim_dt)
    t_arr = np.linspace(0, sim_time, time_steps)

    for idx in range(num_envs):
        omega = np.sqrt(9.81 / heights[idx])
        pos = -0.05
        vel = 0.3
        traj = []
        for _ in range(time_steps):
            acc = omega**2 * pos
            vel += acc * sim_dt
            pos += vel * sim_dt
            traj.append(pos)
        ax3.plot(t_arr, traj, alpha=0.15, color="#2563eb",
                 linewidth=0.8)

    # Nominal trajectory for reference
    omega_nom = np.sqrt(9.81 / nominal_height)
    pos = -0.05
    vel = 0.3
    traj_nom = []
    for _ in range(time_steps):
        acc = omega_nom**2 * pos
        vel += acc * sim_dt
        pos += vel * sim_dt
        traj_nom.append(pos)
    ax3.plot(t_arr, traj_nom, color="#dc2626", linewidth=2.5,
             label="Nominal")
    ax3.set_xlabel("Time (s)")
    ax3.set_ylabel("CoM Position x (m)")
    ax3.set_title(
        "Walking Trajectories Across Randomized Environments")
    ax3.legend()
    ax3.grid(alpha=0.3)

    plt.tight_layout()
    return fig

fig = plot_domain_randomization()
buf = io.BytesIO()
plt.savefig(buf, format="png", dpi=100, bbox_inches="tight")
buf.seek(0)
img_data = base64.b64encode(buf.read()).decode("utf-8")
print(f"data:image/png;base64,{img_data}")
plt.close()
`}
/>

## Why Domain Randomization Works

Why does randomizing parameters lead to a more robust policy? The key insight
is that the policy no longer optimizes for a single environment. Instead, it
optimizes the **expected performance across a distribution** of environments:

$$
\pi^* = \arg\max_\pi \; \mathbb{E}_{\xi \sim P(\xi)}[J(\pi, \xi)]
$$

where $J(\pi, \xi)$ is the return (cumulative reward) of policy $\pi$ in
environment parameterized by $\xi$, and $P(\xi)$ is the randomization
distribution.

By maximizing the average performance over many different environments, the
policy is forced to discover strategies that are **generally effective** rather
than exploiting quirks of any single simulation setup. Think of it like
learning to drive: if you only practice on dry, sunny roads, you may fail in
rain or snow. Training across rain, snow, and sunshine produces a driver who
handles all conditions reasonably well.

## Failure Modes and Trade-offs

Domain randomization requires careful tuning. Two main failure modes exist:

- **Over-conservative behavior**: If the randomization range is too wide,
  the policy must hedge against extreme scenarios. This often produces slow,
  overly cautious walking because the policy cannot commit to aggressive
  strategies that might fail in some corner of the parameter space.

- **Under-randomization**: If the range is too narrow, the policy may
  overfit to simulation-specific features (e.g., perfectly flat ground,
  zero sensor delay) that do not exist on the real robot. The resulting
  policy fails when encountering real-world conditions outside its
  training distribution.

**Practical guidelines** for setting randomization ranges:

1. **Start with physical tolerances**: Set mass ranges within manufacturing
   tolerance (e.g., $\pm 5\%$), friction within measured floor ranges.
2. **Randomize uncertain parameters more**: Parameters that are hard to
   measure accurately (friction coefficients, damping ratios, contact
   stiffness) should have wider ranges than well-characterized ones
   (link masses, geometry).
3. **Iterate based on transfer results**: Deploy to the real robot, observe
   failures, and widen the relevant randomization ranges.

## Simulation: Randomization Range Analysis

The following simulation demonstrates the trade-off between narrow and wide
randomization. We sample environments with three different ranges (narrow,
medium, wide) and compare the walking outcomes.

<CodeEditor
  initialCode={`import math
import random
import matplotlib
matplotlib.use("agg")
import matplotlib.pyplot as plt
import numpy as np
import io, base64

random.seed(42)
rng = np.random.RandomState(42)

def simulate_walking(omega, x0=-0.05, v0=0.3, dt=0.01,
                     duration=0.5):
    """Simulate LIPM walking and return final position."""
    x = x0
    v = v0
    for _ in range(int(duration / dt)):
        a = omega**2 * x
        v += a * dt
        x += v * dt
    return x

def sample_and_simulate(height_range, num_envs=80):
    """Sample environments and simulate walking."""
    nominal_height = 0.8
    lo = nominal_height * (1.0 - height_range)
    hi = nominal_height * (1.0 + height_range)
    heights = rng.uniform(lo, hi, num_envs)
    omegas = np.sqrt(9.81 / heights)
    finals = [simulate_walking(w) for w in omegas]
    return finals

# Three randomization ranges
ranges = {
    "Narrow (5%)": 0.05,
    "Medium (15%)": 0.15,
    "Wide (30%)": 0.30,
}

results = {}
print("=== Randomization Range Analysis ===")
print()

for label, r in ranges.items():
    finals = sample_and_simulate(r)
    results[label] = finals
    mean_val = np.mean(finals)
    std_val = np.std(finals)
    print(f"{label}:")
    print(f"  Final x mean={mean_val:.4f}m, std={std_val:.4f}m")
    print(f"  Range: [{min(finals):.4f}, {max(finals):.4f}]")
    print()

print("Wider randomization -> more variation in outcomes")
print("But policies trained on wider ranges are more robust")
print("to real-world parameter uncertainty.")

# --- Box plot comparison ---
def plot_range_analysis(plot_results):
    """Plot box plots comparing randomization ranges."""
    fig, ax = plt.subplots(figsize=(8, 5))

    labels = list(plot_results.keys())
    data = [plot_results[k] for k in labels]
    colors = ["#2563eb", "#16a34a", "#dc2626"]

    bp = ax.boxplot(
        data, labels=labels, patch_artist=True,
        widths=0.5, showfliers=True,
        flierprops=dict(marker="o", markersize=3, alpha=0.5))

    for patch, color in zip(bp["boxes"], colors):
        patch.set_facecolor(color)
        patch.set_alpha(0.4)
    for median in bp["medians"]:
        median.set_color("black")
        median.set_linewidth(2)

    # Overlay individual points
    for i, (d, color) in enumerate(zip(data, colors)):
        x_jitter = rng.normal(0, 0.04, len(d)) + (i + 1)
        ax.scatter(x_jitter, d, alpha=0.3, s=10, color=color,
                   zorder=3)

    ax.set_ylabel("Final CoM Position x (m)")
    ax.set_title(
        "Effect of Randomization Range on Walking Outcomes")
    ax.grid(axis="y", alpha=0.3)

    # Annotate trade-off
    ax.annotate(
        "Less variation\\nbut may overfit",
        xy=(1, np.median(data[0])),
        xytext=(0.6, np.max(data[2]) * 0.95),
        fontsize=8, ha="center",
        arrowprops=dict(arrowstyle="->", color="gray"))
    ax.annotate(
        "More robust\\nbut conservative",
        xy=(3, np.median(data[2])),
        xytext=(3.4, np.min(data[0]) * 1.05),
        fontsize=8, ha="center",
        arrowprops=dict(arrowstyle="->", color="gray"))

    plt.tight_layout()
    return fig

fig = plot_range_analysis(results)
buf = io.BytesIO()
plt.savefig(buf, format="png", dpi=100, bbox_inches="tight")
buf.seek(0)
img_data = base64.b64encode(buf.read()).decode("utf-8")
print(f"data:image/png;base64,{img_data}")
plt.close()
`}
/>

## System Identification

Rather than randomizing everything, **system identification** measures
the real robot's parameters to improve simulation accuracy.

### Online System ID

Estimate parameters during deployment:

$$
\hat{\xi}_{k+1} = \hat{\xi}_k + K_k(y_k - \hat{y}_k(\hat{\xi}_k))
$$

where $\hat{\xi}$ is the parameter estimate, $y_k$ is the measured output,
and $K_k$ is the adaptation gain.

## Teacher-Student Training

A widely used approach for sim-to-real transfer is **teacher-student
distillation**. The idea is to separate what the policy needs to *know*
from what it can actually *observe* on the real robot.

<TeacherStudentDiagram />

**Phase 1 (Teacher training)**: A teacher network is trained in simulation
with access to **privileged information** — ground-truth values of friction,
mass, terrain shape, and contact forces that are available inside the
simulator but impossible to measure on a real robot. Because the teacher
sees everything, it can learn a near-optimal policy.

**Phase 2 (Student distillation)**: A student network receives only the
**observations available on the real robot** (joint encoders, IMU, commanded
actions). It is trained via supervised learning to mimic the teacher's
actions. The loss function is simply:

$$
\mathcal{L} = \mathbb{E}\left[\|\pi_S(s) - \pi_T(s, \text{priv})\|^2\right]
$$

The student implicitly learns to infer the missing privileged information
from the patterns in its observable inputs. For example, if friction is low
the robot slides more, and the student learns to recognize that sliding
pattern and respond like the teacher would.

## Rapid Motor Adaptation (RMA)

RMA combines the strengths of domain randomization and online adaptation:

1. **Base policy** $\pi(a | s, z)$: takes state and an environment
   embedding $z$ as input
2. **Adaptation module** $\hat{z} = f(s_{t-H:t})$: estimates $z$
   from recent state history

During training (simulation):
- Train base policy with privileged environment information $z$
- Train adaptation module to predict $z$ from state history

During deployment (real robot):
- Adaptation module estimates $z$ online from sensor data
- Base policy acts using estimated $z$

### How RMA Works in Detail

The adaptation module takes a **history of recent observations** — typically
the last $H = 50$ timesteps of joint positions, velocities, and commanded
actions. From this temporal window, it implicitly estimates the current
environment parameters (friction, mass distribution, terrain slope).

The output is an **environment encoding** vector $z \in \mathbb{R}^d$
(typically $d = 8$ to $32$), which is concatenated with the current
observation $s_t$ before being fed into the base policy:

$$
a_t = \pi(s_t, \hat{z}_t), \quad \hat{z}_t = f_\phi(s_{t-H}, \ldots, s_t)
$$

The adaptation module $f_\phi$ is trained with a regression loss to match
the privileged environment encoding $z^*$ that the teacher computes from
ground-truth parameters:

$$
\mathcal{L}_\text{adapt} = \mathbb{E}\left[\|\hat{z}_t - z^*\|^2\right]
$$

**Why does history contain environment information?** Because the robot's
response to the same commanded action differs depending on the physical
parameters. On a slippery floor, the foot slides when pushing off; with
a heavier payload, accelerations are smaller for the same torques. The
adaptation module learns to read these signatures from the observation
history.

<CodeEditor
  initialCode={`import math
import random

# Simplified Rapid Motor Adaptation concept
random.seed(42)

# Environment parameters (the "latent" z)
true_params = {
    "mass_ratio": 1.2,     # 20% heavier than nominal
    "friction": 0.6,       # lower friction
    "motor_strength": 0.9,  # 10% weaker motors
}

# Simulate state history with these parameters
def simulate_step(x, v, force, mass_ratio, friction):
    dt = 0.01
    a = force / (30.0 * mass_ratio) - friction * v * 0.1
    v_new = v + a * dt
    x_new = x + v_new * dt
    return x_new, v_new

# Generate state history
print("=== Rapid Motor Adaptation Demo ===")
print()
print("True environment parameters:")
for k, v in true_params.items():
    print(f"  {k}: {v}")
print()

history = []
x, v = 0.0, 0.0
for t in range(50):
    force = 5.0 * math.sin(t * 0.2)  # probing signal
    x, v = simulate_step(x, v, force,
                          true_params["mass_ratio"],
                          true_params["friction"])
    history.append((x, v, force))

# "Adaptation module": estimate parameters from history
# (In practice, this is a neural network)
# Simple estimation: use response characteristics
responses = [h[1] for h in history[10:]]  # velocities
forces = [h[2] for h in history[10:]]

# Estimate mass ratio from force-acceleration relationship
accels = [(responses[i+1] - responses[i]) / 0.01 for i in range(len(responses)-1)]
avg_response = sum(abs(a) for a in accels) / len(accels)
nominal_response = 0.17  # expected for nominal params
estimated_mass_ratio = nominal_response / avg_response * 1.0

# Estimate friction from velocity decay
vel_decay = sum(abs(responses[i+1]) - abs(responses[i])
                for i in range(len(responses)-1) if abs(forces[i]) < 0.1)

print("Adaptation module estimates:")
print(f"  mass_ratio: {estimated_mass_ratio:.2f} (true: {true_params['mass_ratio']:.2f})")
print()
print("With adaptation, the policy adjusts its behavior:")
print("  - Heavier mass -> stronger push-off")
print("  - Lower friction -> shorter steps")
print("  - Weaker motors -> conservative gait")

# --- RMA adaptation convergence plot ---
import matplotlib
matplotlib.use("agg")
import matplotlib.pyplot as plt
import numpy as np
import io, base64

def plot_rma_convergence():
    """Plot parameter estimation convergence and error."""
    conv_rng = np.random.RandomState(42)
    num_steps = 200

    # True parameters
    true_mass = 1.2
    true_friction = 0.6

    # Simulated adaptation: exponential convergence with noise
    time_arr = np.arange(num_steps)
    tau_mass = 30.0
    tau_friction = 40.0
    noise_scale = 0.03

    est_mass = (
        true_mass
        + (1.0 - true_mass) * np.exp(-time_arr / tau_mass)
        + conv_rng.normal(0, noise_scale, num_steps)
        * np.exp(-time_arr / (tau_mass * 2)))
    est_friction = (
        true_friction
        + (0.8 - true_friction)
        * np.exp(-time_arr / tau_friction)
        + conv_rng.normal(0, noise_scale, num_steps)
        * np.exp(-time_arr / (tau_friction * 2)))

    fig, axes = plt.subplots(2, 1, figsize=(8, 6))

    # Top: true vs estimated values
    ax = axes[0]
    ax.plot(time_arr, est_mass, color="#2563eb", alpha=0.8,
            linewidth=1.2, label="Estimated mass ratio")
    ax.axhline(y=true_mass, color="#2563eb", linestyle="--",
               linewidth=1.5, label="True mass ratio (1.2)")
    ax.plot(time_arr, est_friction, color="#dc2626", alpha=0.8,
            linewidth=1.2, label="Estimated friction")
    ax.axhline(y=true_friction, color="#dc2626", linestyle="--",
               linewidth=1.5, label="True friction (0.6)")
    ax.set_ylabel("Parameter Value")
    ax.set_title("RMA: Parameter Estimation Convergence")
    ax.legend(loc="center right", fontsize=8)
    ax.grid(alpha=0.3)

    # Bottom: estimation error
    ax2 = axes[1]
    err_mass = np.abs(est_mass - true_mass)
    err_friction = np.abs(est_friction - true_friction)
    ax2.plot(time_arr, err_mass, color="#2563eb", alpha=0.8,
             linewidth=1.2, label="Mass ratio error")
    ax2.plot(time_arr, err_friction, color="#dc2626", alpha=0.8,
             linewidth=1.2, label="Friction error")
    ax2.set_xlabel("Time Step")
    ax2.set_ylabel("Absolute Error")
    ax2.set_title("Estimation Error Over Time")
    ax2.legend(fontsize=8)
    ax2.grid(alpha=0.3)

    plt.tight_layout()
    return fig

fig = plot_rma_convergence()
buf = io.BytesIO()
plt.savefig(buf, format="png", dpi=100, bbox_inches="tight")
buf.seek(0)
img_data = base64.b64encode(buf.read()).decode("utf-8")
print(f"data:image/png;base64,{img_data}")
plt.close()
`}
/>

## Practical Deployment

### Contact Modeling

Contact modeling is the single biggest source of the sim-to-real gap. Real
contact involves soft deformable materials, complex foot geometries, and
surface irregularities that are extremely difficult to simulate faithfully.
Most simulators use rigid-body contact with simplified friction cones,
which diverges from reality especially during heel-strike and toe-off phases.

### Actuator Modeling

Real motors have characteristics that are hard to capture in simulation:
thermal effects (motors weaken as they heat up), nonlinear friction
(stiction, Coulomb friction, viscous damping), communication delays between
the controller and motor drivers, and current/torque limits that vary with
speed. These effects compound during dynamic locomotion.

### Sensor Noise and Latency

Real IMUs have gyroscope drift, accelerometer bias, and magnetometer
interference. Joint encoders have quantization noise. All sensor readings
arrive with latency (typically 1-10 ms). Training with simulated sensor
noise and delays is essential, but matching the real noise characteristics
exactly is difficult.

### Engineering Effort

Building a simulation environment that transfers well to a specific robot
typically requires significant engineering effort — often several months
for a new platform. This includes careful CAD-to-URDF conversion, actuator
model tuning, sensor noise characterization, and iterative refinement based
on real-robot experiments.

## Key Results in Sim-to-Real Walking

| Year | System | Method | Achievement |
|------|--------|--------|-------------|
| 2018 | Cassie | Domain randomization | Outdoor walking |
| 2020 | ANYmal | Teacher-student | Blind locomotion |
| 2023 | Digit | RMA | Multi-terrain |
| 2024 | Various | Large-scale RL | Agile humanoid locomotion |

## Connections to Other Chapters

- **Reinforcement Learning (Ch 11)**: Sim-to-real transfer is almost always
  used with RL-trained policies. The techniques in this chapter (domain
  randomization, teacher-student, RMA) are the standard methods for
  deploying RL locomotion policies on real hardware.
- **System identification vs. domain randomization**: Traditional robotics
  relies on careful system identification to build accurate models.
  Modern RL-based approaches use domain randomization instead, trading
  model accuracy for policy robustness.
- **Model Predictive Control (Ch 10)**: MPC-based approaches avoid much of
  the sim-to-real gap because they re-plan online using a model that can
  be updated with real-time state estimation. However, MPC is limited to
  simpler dynamics models and shorter horizons compared to learned policies.

## References

- J. Tobin et al., "[Domain Randomization for Transferring Deep Neural Networks from Simulation to the Real World](https://arxiv.org/abs/1703.06907)," *Proc. IROS*, 2017.
- I. Radosavovic et al., "[Real-World Humanoid Locomotion with Reinforcement Learning](https://arxiv.org/abs/2303.03381)," *Science Robotics*, 2024.
- A. Kumar et al., "[RMA: Rapid Motor Adaptation for Legged Robots](https://arxiv.org/abs/2107.04034)," *RSS*, 2021.
- J. Lee et al., "[Learning Quadrupedal Locomotion over Challenging Terrain](https://arxiv.org/abs/2010.11251)," *Science Robotics*, 2020.

<InteractiveDemo title="Domain Randomization Visualization">
  <p className="text-sm text-gray-500">
    Interactive domain randomization parameter visualization coming soon.
  </p>
</InteractiveDemo>
